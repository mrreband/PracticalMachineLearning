#Practical Machine Learning - Course Project 
This project aims to demonstrate the predictive power of Machine Learning techniques, by processing the provided wearable-technology data set, and using accelerometer data to predict specific manners in which exercises were performed.  

Note that the analysis below requires the following libraries: 

- caret
- randomForest
- corrplot


###Data 
The provided data set can be found [here](http://groupware.les.inf.puc-rio.br/har).  It consists of 19,622 observations of 160 variables.  

```{r echo=FALSE,message=FALSE}
#download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","pml-training.csv")
#download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","pml-testing.csv")
pmlTraining <-  read.csv("pml-training.csv", sep = ",", header = TRUE)
pmlTesting <- read.csv("pml-testing.csv", sep=",", header = TRUE)
```

67 of these variables contain N/A values for more than 95% of the observations, so they will be excluded from further analysis:  
```{r}
naCounts <-sapply(pmlTraining, function(y) sum(length(which(is.na(y)))))
pmlTraining <- pmlTraining[,naCounts / nrow(pmlTraining) < 0.95]
```

An additional 42 variables are factor variables with very little variance, so they will be excluded as well.  We can also exclude the "x" and "timestamp" variables, as they are for identification purposes and do not provide any predictive power. 

We will keep the classe variable as it is the dependent variable to be predicted. 

```{r}
#exclude factor columns
isFactor <- sapply(pmlTraining, function(y) is.factor(y))
isFactor["classe"] = FALSE
pmlTraining <- pmlTraining[,!isFactor]

#exclude x and timestamp variables
pmlTraining <- pmlTraining[, !(colnames(pmlTraining) %in% c("x", "raw_timestamp_part_1", "raw_timestamp_part_2"))]
```

The resulting dataset now contains 19,622 complete observations of 55 variables.

###Correlation

Below are several statements to generate correlation matrixes for each sensor location (belt, arm, forearm, dumbbell).  In the interest of brevity, the results or each correlation plot is not explicitly included -- they can be reproduced independently if desired.   

The correlation plots show that most data points are statiscally uncorrelated with similar data points, suggesting that they can each provide additional predictive power in our final model.  Several exceptions to this: 

- "roll\_belt" vs "total\_accel\_belt"
- "pitch\_belt" vs "accel\_belt\_x", "accel\_belt\_z", "magnet\_belt\_x"
- "gyros\_arm\_x" vs "gyros\_arm\_y"
- "magnet\_arm\_y" vs "accel\_arm\_x", "accel\_arm\_z", "magnet\_arm\_x", "magnet\_arm\_z"
- "magnet\_arm\_z" vs "accel\_arm\_x", "magnet\_arm\_x", "magnet\_arm\_y"
- "gyros\_dumbbell\_x" vs "gyros\_dumbbell\_z"

```{r eval=FALSE}
require(corrplot)
corrplot(cor(pmlTraining[,grep("belt", names(pmlTraining))]), method="pie")
corrplot(cor(pmlTraining[,grep("_arm_", names(pmlTraining))]), method="pie")
corrplot(cor(pmlTraining[,grep("forearm", names(pmlTraining))]), method="pie")
corrplot(cor(pmlTraining[,grep("dumbbell", names(pmlTraining))]), method="pie")
```

In light of this, we will also exclude pitch\_belt, magnet\_arm\_y, and magnet\_arm\_z variables. 

```{r echo=FALSE}
pmlTraining <- pmlTraining[, !(colnames(pmlTraining) %in% c("pitch_belt", "magnet_arm_y", "magnet_arm_z"))]
#pmlCols  <- names(pmlTraining)
#pmlTesting <- pmlTesting[,pmlCols[1:51]]
```


###Training and Predicting
We will reserve 30% of the training data set to use for validation purposes, so that we can get an idea of the efficacy of our algorithm before attempting to apply it to the testing data set.  

```{r echo=FALSE, message=F, warning=F}
require(caret)
```

```{r}
set.seed(7231)
inTrain = createDataPartition(pmlTraining$classe, p = 0.7)[[1]]
trainData <- pmlTraining[inTrain,]
validationData <- pmlTraining[-inTrain,]
```

Next, we can use the training data set to build a random forest predictive model.  We can then use this model to predict the *'classe'* values in our validation data set.  

```{r message=F, warning=F}
mod <- train(classe~.,method="rf",data=trainData, allowParallel=TRUE, trControl = trainControl(method = "cv", number = 3), preProcess="pca")
validationPred <- predict(mod, validationData)
```

###Results 
Below is the confusion matrix produced when comparing the predicted values to the actual values in the validation data set.  The random forest algorithm proves to be extremely accurate: 

```{r}
confusionMatrix(validationPred, validationData$classe)
```

We can now use our algorithm to make our final test data predictions:
```{r}
testPred <- predict(mod, newdata=pmlTesting)
testPred
```

```{r echo=FALSE}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("predictions/problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

answers <- as.character(testPred)
pml_write_files(answers)
```